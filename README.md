# Inference Engine Deployment
This is a placeholder readme for an active project to build a complete end-to-end model serving engine and support pipeline on my local network & GPU cluster. More documentation will come once this gets to a more complete state.

#### Deployed Services
 - TorchServe
 - Prometheus
 - Grafana
 - DCGM-Exporter
 - FastAPI
 - NGINX

#### Monitoring
![grafana_dashboard](https://raw.githubusercontent.com/andersonvc/onprem-inference-deployment/main/docs/grafana_dashboard.png?raw=true)


#### CLient Serving & Model Management API
![api_interface](https://raw.githubusercontent.com/andersonvc/onprem-inference-deployment/main/docs/api_interface.png?raw=true)



#### Useful Links
https://ssl-config.mozilla.org/ --Configuring NGINX
https://github.com/obynio/certbot-plugin-gandi --Certbot Setup for Gandi webhost
https://webrtc.github.io/samples/ --WebRTC Samples
