# Inference Engine Deployment
This is a placeholder readme for an active project to build a complete end-to-end model serving engine and support pipeline on my local network & GPU cluster. More documentation will come once this gets to a more complete state.

#### Deployed Services
 - TorchServe
 - Prometheus
 - Grafana
 - DCGM-Exporter
 - FastAPI

#### Monitoring
![grafana_dashboard](https://raw.githubusercontent.com/andersonvc/onprem-inference-deployment/main/docs/grafana_dashboard.png?raw=true)
